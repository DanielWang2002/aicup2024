{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "import xgboost as xgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 導入欲預測的資料表"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "139854\n"
     ]
    }
   ],
   "source": [
    "# Load the uploaded dataset\n",
    "file_path = '../concat.csv'\n",
    "data = pd.read_csv(file_path)\n",
    "# 地區分類\n",
    "data['Class'] = data['LocationCode'].apply(lambda i: 1 if i <= 14 else (2 if i <= 16 else 3))\n",
    "data = data.sort_values(by=['Month', 'Day', 'Hour', 'Minute']).reset_index(drop=True)\n",
    "print(len(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 轉換為一天中的分鐘數\n",
    "data['MinutesOfDay'] = data['Hour'] * 60 + data['Minute']\n",
    "\n",
    "# 週期性編碼\n",
    "data['Time_sin'] = np.sin(2 * np.pi * data['MinutesOfDay'] / (24 * 60))\n",
    "data['Time_cos'] = np.cos(2 * np.pi * data['MinutesOfDay'] / (24 * 60))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Convert all data to float and remove rows with non-convertible values\n",
    "for column in data.columns:\n",
    "    if column not in ['Month', 'Day', 'Hour', 'Minute', 'Class']:\n",
    "        try:\n",
    "            data[column] = data[column].astype(float)\n",
    "        except ValueError:\n",
    "            data = data.drop(columns=[column])\n",
    "\n",
    "# Remove rows with NaN values created during the conversion\n",
    "data = data.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler1 = StandardScaler()\n",
    "data['Sunlight(Lux)_normalized'] = scaler1.fit_transform(data[['Sunlight']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "solar_path = './solar_angles_10min_new.csv'\n",
    "solar_data = pd.read_csv(solar_path)\n",
    "solar_data_unique = solar_data.drop_duplicates(subset=['Month', 'Day', 'Hour', 'Minute'])\n",
    "\n",
    "# 左合併，確保原資料長度不變\n",
    "data = pd.merge(\n",
    "    data, \n",
    "    solar_data_unique, \n",
    "    on=['Month', 'Day', 'Hour', 'Minute'], \n",
    "    how='left'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Temperature', 'Humidity', 'Sunlight', 'Power', 'Time_cos', 'Sunlight(Lux)_normalized', '仰角', 'Class']\n",
      "與(Power(mW))的相關性：\n",
      "Power                       1.000000\n",
      "Sunlight(Lux)_normalized    0.942155\n",
      "Sunlight                    0.942155\n",
      "Temperature                 0.688920\n",
      "仰角                          0.455086\n",
      "Time_sin                    0.091064\n",
      "Month                       0.079428\n",
      "Day                         0.022828\n",
      "Class                       0.006967\n",
      "Minute                      0.000316\n",
      "Pressure                   -0.048836\n",
      "Hour                       -0.064903\n",
      "MinutesOfDay               -0.064993\n",
      "LocationCode               -0.069048\n",
      "WindSpeed                  -0.109665\n",
      "Time_cos                   -0.444742\n",
      "Humidity                   -0.669537\n",
      "Name: Power, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Step 2: Calculate correlations with the target\n",
    "target = 'Power'\n",
    "excluded_columns = [\"AvgTemp\"]\n",
    "remaining_columns = [col for col in data.columns if col not in excluded_columns]\n",
    "data = data[remaining_columns]\n",
    "correlation = data.corr()\n",
    "\n",
    "# Find features with a correlation > 0.3 with \"Power(mW)\"\n",
    "features_high_corr = correlation[(abs(correlation[\"Power\"]) > 0.4)].index\n",
    "\n",
    "# Exclude the target itself from the features\n",
    "features_high_corr = [feat for feat in features_high_corr]\n",
    "features_high_corr.append('Class')\n",
    "\n",
    "# Display the selected features\n",
    "print(features_high_corr)\n",
    "\n",
    "print(\"與(Power(mW))的相關性：\")\n",
    "print(correlation[target].sort_values(ascending=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hour', 'Sunlight(Lux)_normalized', '仰角', 'Class', 'Power', 'Time_sin']\n",
      "        Hour  Sunlight(Lux)_normalized     仰角  Class      Power  Time_sin\n",
      "0          6                 -0.740348   0.00      1   0.002500  0.991445\n",
      "1          6                 -0.740312   0.00      3   0.007500  0.991445\n",
      "2          6                 -0.736501   0.74      1   0.016000  0.984808\n",
      "3          6                 -0.735784   0.74      3   0.017000  0.984808\n",
      "4          6                 -0.730715   2.84      1   0.046667  0.976296\n",
      "...      ...                       ...    ...    ...        ...       ...\n",
      "139849    13                 -0.449515  42.38      1  18.795000 -0.461749\n",
      "139850    14                 -0.565218  40.93      1   6.646000 -0.500000\n",
      "139851    14                 -0.620042  39.40      1   3.003750 -0.537300\n",
      "139852    14                 -0.628694  37.78      1   2.440000 -0.573576\n",
      "139853    14                 -0.623517  36.08      1   2.590000 -0.608761\n",
      "\n",
      "[139380 rows x 6 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_394014/2603739136.py:6: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_selected.dropna(inplace=True)\n"
     ]
    }
   ],
   "source": [
    "# features_high_corr = [feat for feat in features_high_corr if feat not in ['Sunlight(Lux)', 'Hour', 'feature_Temperature', 'Class', target]]\n",
    "features_high_corr = ['Hour', 'Sunlight(Lux)_normalized', '仰角', 'Class', 'Power', 'Time_sin']\n",
    "print(features_high_corr)\n",
    "\n",
    "df_selected = data[features_high_corr]\n",
    "df_selected.dropna(inplace=True)\n",
    "df_selected.reset_index(drop=True)\n",
    "print(df_selected)\n",
    "\n",
    "# Split dataset based on the 'Class' column into separate groups\n",
    "class_datasets = {class_label: group.drop(columns=[\"Class\"]) for class_label, group in df_selected.groupby(\"Class\")}\n",
    "\n",
    "features_high_corr = ['Sunlight(Lux)_normalized', '仰角', 'Time_sin']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split each class dataset into train and test sets (8:2 ratio)\n",
    "split_data = {\n",
    "    class_label: train_test_split(data, test_size=0.2, random_state=42, shuffle=False)\n",
    "    for class_label, data in class_datasets.items()\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "# Prepare datasets for training and testing\n",
    "def prepare_dataloader(data, batch_size=128):\n",
    "    feature_columns = features_high_corr\n",
    "    X = torch.tensor(data[feature_columns].values, dtype=torch.float32).unsqueeze(1).to(device)  # Add sequence dimension\n",
    "    y = torch.tensor(data[target].values, dtype=torch.float32).to(device)\n",
    "    dataset = TensorDataset(X, y)\n",
    "    return DataLoader(dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSTM 模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定義 Attention 層\n",
    "class Attention(nn.Module):\n",
    "    def __init__(self, hidden_layer_size):\n",
    "        super(Attention, self).__init__()\n",
    "        self.attention = nn.Linear(hidden_layer_size, 1, bias=False)\n",
    "    \n",
    "    def forward(self, lstm_output):\n",
    "        # lstm_output: (batch_size, time_steps, hidden_layer_size)\n",
    "        attention_weights = torch.softmax(self.attention(lstm_output), dim=1)\n",
    "        # attention_weights: (batch_size, time_steps, 1)\n",
    "        weighted_output = lstm_output * attention_weights  # (batch_size, time_steps, hidden_layer_size)\n",
    "        output = torch.sum(weighted_output, dim=1)  # 加權後的輸出 (batch_size, hidden_layer_size)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LSTM Model\n",
    "class LSTM_MLP_Model(nn.Module):\n",
    "    def __init__(self, input_size=5, hidden_layer_size=256, output_size=1, mlp_hidden_size_1=256, mlp_hidden_size_2=256):\n",
    "        super(LSTM_MLP_Model, self).__init__()\n",
    "        self.lstm = nn.LSTM(input_size, hidden_layer_size, num_layers=12, batch_first=True)\n",
    "        self.hidden_layer_size = hidden_layer_size\n",
    "        self.attention = Attention(self.hidden_layer_size)\n",
    "        # self.softmax = nn.Softmax()\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(hidden_layer_size, mlp_hidden_size_1),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(mlp_hidden_size_1, mlp_hidden_size_2),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(mlp_hidden_size_2, output_size)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        lstm_out, _ = self.lstm(x)  # LSTM 輸出\n",
    "        # lstm_out = lstm_out[:, -1, :]  # 只取最後一個時間步的輸出\n",
    "        attention_out = self.attention(lstm_out)\n",
    "        predictions = self.mlp(attention_out)  # 輸入到 MLP\n",
    "        return predictions, lstm_out[:, -1, :]  # 同時返回 MLP 的最終預測和 LSTM 的特徵"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 建構集成模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize models, dataloaders, and training configurations\n",
    "models = {}\n",
    "dataloaders = {}\n",
    "hidden_layer_size = 256\n",
    "mlp_hidden_size_1 = 512\n",
    "mlp_hidden_size_2 = 1024\n",
    "learning_rate = 0.001\n",
    "output_size = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training function\n",
    "def train_model(model, xgb_model, train_loader, criterion, optimizer, num_epochs=1000):\n",
    "    model.train()\n",
    "    for epoch in range(num_epochs):\n",
    "        running_loss = 0.0\n",
    "        for inputs, targets in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            outputs, lstm_out = model(inputs)\n",
    "            # xgb 訓練資料\n",
    "            lstm_train_features_np = lstm_out.cpu().detach().numpy()\n",
    "            targets_np = targets.cpu().detach().numpy()\n",
    "            # XGBoost 模型訓練\n",
    "            xgb_model.fit(lstm_train_features_np, targets_np)\n",
    "            # XGBoost 模型預測\n",
    "            y_pred_xgb = xgb_model.predict(lstm_train_features_np)\n",
    "            y_pred_xgb_tensor = torch.tensor(y_pred_xgb, dtype=torch.float32).to(device)\n",
    "            loss = criterion(outputs.squeeze(), targets)\n",
    "            # 合併損失值\n",
    "            loss_lstm = criterion(outputs.squeeze(), targets)\n",
    "            loss_xgb = criterion(y_pred_xgb_tensor, targets)\n",
    "            total_loss = (loss_lstm+loss_xgb)/2\n",
    "            total_loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {running_loss/len(train_loader):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing function\n",
    "def test_model(model, xgb_model, test_loader, criterion):\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for inputs, targets in test_loader:\n",
    "            outputs, lstm_out = model(inputs)\n",
    "            # xgb 訓練資料\n",
    "            lstm_train_features_np = lstm_out.cpu().detach().numpy()\n",
    "            targets_np = targets.cpu().detach().numpy()\n",
    "            # XGBoost 模型訓練\n",
    "            xgb_model.fit(lstm_train_features_np, targets_np)\n",
    "            # XGBoost 模型預測\n",
    "            y_pred_xgb = xgb_model.predict(lstm_train_features_np)\n",
    "            y_pred_xgb_tensor = torch.tensor(y_pred_xgb, dtype=torch.float32).to(device)\n",
    "            loss = criterion(outputs.squeeze(), targets)\n",
    "            # 合併損失值\n",
    "            loss_lstm = criterion(outputs.squeeze(), targets)\n",
    "            loss_xgb = criterion(y_pred_xgb_tensor, targets)\n",
    "            total_loss = (loss_lstm+loss_xgb)/2\n",
    "            total_loss += total_loss\n",
    "    avg_loss = total_loss / len(test_loader)\n",
    "    print(f\"Test Loss: {avg_loss:.4f}\")\n",
    "    return avg_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model for Class 1...\n",
      "Epoch 1/600, Loss: 202.0688\n",
      "Epoch 2/600, Loss: 112.1050\n",
      "Epoch 3/600, Loss: 82.4126\n",
      "Epoch 4/600, Loss: 56.5130\n",
      "Epoch 5/600, Loss: 55.5298\n",
      "Epoch 6/600, Loss: 55.0455\n",
      "Epoch 7/600, Loss: 55.1858\n",
      "Epoch 8/600, Loss: 53.9547\n",
      "Epoch 9/600, Loss: 54.8556\n",
      "Epoch 10/600, Loss: 54.2799\n",
      "Epoch 11/600, Loss: 54.4843\n",
      "Epoch 12/600, Loss: 54.1061\n",
      "Epoch 13/600, Loss: 51.7504\n",
      "Epoch 14/600, Loss: 52.9149\n",
      "Epoch 15/600, Loss: 53.4212\n",
      "Epoch 16/600, Loss: 53.1474\n",
      "Epoch 17/600, Loss: 51.7263\n",
      "Epoch 18/600, Loss: 52.3462\n",
      "Epoch 19/600, Loss: 52.0606\n",
      "Epoch 20/600, Loss: 52.9303\n",
      "Epoch 21/600, Loss: 50.5329\n",
      "Epoch 22/600, Loss: 50.8933\n",
      "Epoch 23/600, Loss: 51.8005\n",
      "Epoch 24/600, Loss: 51.8331\n",
      "Epoch 25/600, Loss: 50.8319\n",
      "Epoch 26/600, Loss: 50.3227\n",
      "Epoch 27/600, Loss: 50.7622\n",
      "Epoch 28/600, Loss: 50.3748\n",
      "Epoch 29/600, Loss: 50.5527\n",
      "Epoch 30/600, Loss: 51.9147\n",
      "Epoch 31/600, Loss: 49.7594\n",
      "Epoch 32/600, Loss: 53.8107\n",
      "Epoch 33/600, Loss: 50.0825\n",
      "Epoch 34/600, Loss: 49.9271\n",
      "Epoch 35/600, Loss: 50.1828\n",
      "Epoch 36/600, Loss: 50.5920\n",
      "Epoch 37/600, Loss: 50.3455\n",
      "Epoch 38/600, Loss: 50.4699\n",
      "Epoch 39/600, Loss: 49.7244\n",
      "Epoch 40/600, Loss: 49.6617\n",
      "Epoch 41/600, Loss: 49.7656\n",
      "Epoch 42/600, Loss: 50.0523\n",
      "Epoch 43/600, Loss: 49.6057\n",
      "Epoch 44/600, Loss: 48.9167\n",
      "Epoch 45/600, Loss: 49.4515\n",
      "Epoch 46/600, Loss: 48.5679\n",
      "Epoch 47/600, Loss: 49.8467\n",
      "Epoch 48/600, Loss: 48.9028\n",
      "Epoch 49/600, Loss: 49.5255\n",
      "Epoch 50/600, Loss: 49.3140\n",
      "Epoch 51/600, Loss: 49.1629\n",
      "Epoch 52/600, Loss: 48.6045\n",
      "Epoch 53/600, Loss: 49.4566\n",
      "Epoch 54/600, Loss: 48.2330\n",
      "Epoch 55/600, Loss: 49.0277\n",
      "Epoch 56/600, Loss: 48.4747\n",
      "Epoch 57/600, Loss: 49.2386\n",
      "Epoch 58/600, Loss: 48.0701\n",
      "Epoch 59/600, Loss: 48.7187\n",
      "Epoch 60/600, Loss: 48.5578\n",
      "Epoch 61/600, Loss: 49.5482\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[34], line 36\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[38;5;66;03m# Train the model\u001b[39;00m\n\u001b[1;32m     35\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTraining model for Class \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mclass_label\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 36\u001b[0m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodels\u001b[49m\u001b[43m[\u001b[49m\u001b[43mclass_label\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mxgb_models\u001b[49m\u001b[43m[\u001b[49m\u001b[43mclass_label\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataloaders\u001b[49m\u001b[43m[\u001b[49m\u001b[43mclass_label\u001b[49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtrain\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     38\u001b[0m \u001b[38;5;66;03m# Test the model\u001b[39;00m\n\u001b[1;32m     39\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTesting model for Class \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mclass_label\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[32], line 13\u001b[0m, in \u001b[0;36mtrain_model\u001b[0;34m(model, xgb_model, train_loader, criterion, optimizer, num_epochs)\u001b[0m\n\u001b[1;32m     11\u001b[0m targets_np \u001b[38;5;241m=\u001b[39m targets\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mnumpy()\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m# XGBoost 模型訓練\u001b[39;00m\n\u001b[0;32m---> 13\u001b[0m \u001b[43mxgb_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlstm_train_features_np\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtargets_np\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;66;03m# XGBoost 模型預測\u001b[39;00m\n\u001b[1;32m     15\u001b[0m y_pred_xgb \u001b[38;5;241m=\u001b[39m xgb_model\u001b[38;5;241m.\u001b[39mpredict(lstm_train_features_np)\n",
      "File \u001b[0;32m~/miniconda3/envs/aicup/lib/python3.11/site-packages/xgboost/core.py:726\u001b[0m, in \u001b[0;36mrequire_keyword_args.<locals>.throw_if.<locals>.inner_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    724\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m k, arg \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(sig\u001b[38;5;241m.\u001b[39mparameters, args):\n\u001b[1;32m    725\u001b[0m     kwargs[k] \u001b[38;5;241m=\u001b[39m arg\n\u001b[0;32m--> 726\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/aicup/lib/python3.11/site-packages/xgboost/sklearn.py:1108\u001b[0m, in \u001b[0;36mXGBModel.fit\u001b[0;34m(self, X, y, sample_weight, base_margin, eval_set, verbose, xgb_model, sample_weight_eval_set, base_margin_eval_set, feature_weights)\u001b[0m\n\u001b[1;32m   1105\u001b[0m     obj \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1107\u001b[0m model, metric, params \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_configure_fit(xgb_model, params)\n\u001b[0;32m-> 1108\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_Booster \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1109\u001b[0m \u001b[43m    \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1110\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_dmatrix\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1111\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_num_boosting_rounds\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1112\u001b[0m \u001b[43m    \u001b[49m\u001b[43mevals\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mevals\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1113\u001b[0m \u001b[43m    \u001b[49m\u001b[43mearly_stopping_rounds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mearly_stopping_rounds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1114\u001b[0m \u001b[43m    \u001b[49m\u001b[43mevals_result\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mevals_result\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1115\u001b[0m \u001b[43m    \u001b[49m\u001b[43mobj\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1116\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcustom_metric\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmetric\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1117\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1118\u001b[0m \u001b[43m    \u001b[49m\u001b[43mxgb_model\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1119\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1120\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1122\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_set_evaluation_result(evals_result)\n\u001b[1;32m   1123\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "File \u001b[0;32m~/miniconda3/envs/aicup/lib/python3.11/site-packages/xgboost/core.py:726\u001b[0m, in \u001b[0;36mrequire_keyword_args.<locals>.throw_if.<locals>.inner_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    724\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m k, arg \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(sig\u001b[38;5;241m.\u001b[39mparameters, args):\n\u001b[1;32m    725\u001b[0m     kwargs[k] \u001b[38;5;241m=\u001b[39m arg\n\u001b[0;32m--> 726\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/aicup/lib/python3.11/site-packages/xgboost/training.py:181\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(params, dtrain, num_boost_round, evals, obj, feval, maximize, early_stopping_rounds, evals_result, verbose_eval, xgb_model, callbacks, custom_metric)\u001b[0m\n\u001b[1;32m    179\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cb_container\u001b[38;5;241m.\u001b[39mbefore_iteration(bst, i, dtrain, evals):\n\u001b[1;32m    180\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m--> 181\u001b[0m \u001b[43mbst\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mupdate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdtrain\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43miteration\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfobj\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    182\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cb_container\u001b[38;5;241m.\u001b[39mafter_iteration(bst, i, dtrain, evals):\n\u001b[1;32m    183\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/aicup/lib/python3.11/site-packages/xgboost/core.py:2101\u001b[0m, in \u001b[0;36mBooster.update\u001b[0;34m(self, dtrain, iteration, fobj)\u001b[0m\n\u001b[1;32m   2097\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_assign_dmatrix_features(dtrain)\n\u001b[1;32m   2099\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m fobj \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   2100\u001b[0m     _check_call(\n\u001b[0;32m-> 2101\u001b[0m         \u001b[43m_LIB\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mXGBoosterUpdateOneIter\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2102\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mctypes\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mc_int\u001b[49m\u001b[43m(\u001b[49m\u001b[43miteration\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtrain\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhandle\u001b[49m\n\u001b[1;32m   2103\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2104\u001b[0m     )\n\u001b[1;32m   2105\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   2106\u001b[0m     pred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpredict(dtrain, output_margin\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, training\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "models = {}\n",
    "dataloaders = {}\n",
    "num_epochs = 600\n",
    "\n",
    "xgb_models = {}\n",
    "\n",
    "for class_label, (train_data, test_data) in split_data.items():\n",
    "    input_size = len(features_high_corr)\n",
    "\n",
    "    # Initialize the model and move it to the correct device (GPU or CPU)\n",
    "    models[class_label] = LSTM_MLP_Model(\n",
    "        input_size=input_size,\n",
    "        hidden_layer_size=hidden_layer_size,\n",
    "        mlp_hidden_size_1=mlp_hidden_size_1,\n",
    "        mlp_hidden_size_2=mlp_hidden_size_2,\n",
    "        output_size=output_size\n",
    "    ).to(device)\n",
    "    # 初始化 XGBoost 模型\n",
    "    xgb_models[class_label] = xgb.XGBRegressor(objective=\"reg:absoluteerror\", n_estimators=20, max_depth=8, learning_rate=0.01,\n",
    "    # tree_method=\"gpu_hist\",  # 使用 GPU 加速\n",
    "    # predictor=\"gpu_predictor\"  # 使用 GPU 進行預測\n",
    "    )\n",
    "    \n",
    "    # Prepare data loaders for training and testing\n",
    "    dataloaders[class_label] = {\n",
    "        \"train\": prepare_dataloader(train_data, batch_size=1024),\n",
    "        \"test\": prepare_dataloader(test_data, batch_size=128)\n",
    "    }\n",
    "\n",
    "    # Define the loss function and optimizer\n",
    "    criterion = nn.L1Loss()\n",
    "    optimizer = torch.optim.Adam(models[class_label].parameters(), lr=learning_rate)\n",
    "\n",
    "    # Train the model\n",
    "    print(f\"Training model for Class {class_label}...\")\n",
    "    train_model(models[class_label], xgb_models[class_label], dataloaders[class_label][\"train\"], criterion, optimizer, num_epochs)\n",
    "\n",
    "    # Test the model\n",
    "    print(f\"Testing model for Class {class_label}...\")\n",
    "    test_loss = test_model(models[class_label], xgb_models[class_label], dataloaders[class_label][\"test\"], criterion)\n",
    "\n",
    "    print(f\"Class {class_label} Test Loss: {test_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSTM+MLP 驗證"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 欄位名與目標欄位\n",
    "feature_columns = features_high_corr\n",
    "\n",
    "# 讀取檔案\n",
    "file_path = '../比賽資料/36_TestSet_SubmissionTemplate/match_merge1.csv'\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "\n",
    "# 預測發電量\n",
    "predictions = []\n",
    "\n",
    "for class_label, model in models.items():\n",
    "    # 選擇當前類別的數據\n",
    "    class_data = df[df[\"Class\"] == class_label]\n",
    "    if class_data.empty:\n",
    "        continue  # 如果該類別無數據，跳過\n",
    "\n",
    "    # 提取特徵\n",
    "    features = class_data[feature_columns]\n",
    "    \n",
    "    # 將特徵轉換為 PyTorch 張量\n",
    "    features_tensor = torch.tensor(features.values, dtype=torch.float32).to(device)\n",
    "    \n",
    "    # 模型預測\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        outputs, lstm_out = model(features_tensor.unsqueeze(1))  # 添加序列維度\n",
    "\n",
    "        # xgb 訓練資料\n",
    "        lstm_val_features_np = lstm_out.cpu().detach().numpy()\n",
    "        y_pred_xgb = xgb_models[class_label].predict(lstm_val_features_np)\n",
    "\n",
    "        lstm_outputs = outputs.squeeze().cpu().numpy()  # LSTM 預測值\n",
    "        final_predictions = (lstm_outputs*0.3 + y_pred_xgb*0.7) / 2  # 取平均\n",
    "\n",
    "        # 儲存到 DataFrame\n",
    "        class_data[\"答案\"] = final_predictions\n",
    "    \n",
    "    # 紀錄結果\n",
    "    predictions.append(class_data[[\"序號\", \"答案\"]])\n",
    "\n",
    "# 合併所有類別的預測結果\n",
    "result = pd.concat(predictions)\n",
    "\n",
    "# 儲存預測結果\n",
    "output_path = '../比賽資料/36_TestSet_SubmissionTemplate/up_lstm_xgb.csv'  # 替換為實際輸出檔案路徑\n",
    "result.to_csv(output_path, index=False)\n",
    "\n",
    "print(f\"預測完成，結果已儲存到 {output_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 檢查每個欄位中缺失值的數量\n",
    "missing_columns = result.isnull().sum()\n",
    "\n",
    "# 篩選出有缺失值的欄位\n",
    "missing_columns = missing_columns[missing_columns > 0]\n",
    "\n",
    "# 顯示有缺失值的欄位及其對應的缺失數量\n",
    "print(\"欄位及缺失值數量：\")\n",
    "print(missing_columns)\n",
    "\n",
    "# 找出有缺失值的行的索引\n",
    "print(\"\\n包含缺失值的索引：\")\n",
    "missing_rows = result[result.isnull().any(axis=1)]\n",
    "print((missing_rows.index))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nchu_1131_finance",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
